name: Deploy Blog to GitHub Pages

on:
  schedule:
    - cron: '*/30 * * * *'
  push:
    branches:
      - main
    paths:
      - 'astro-blog/**'
  workflow_dispatch:
  repository_dispatch:
    types: [blog-publish, blog-post-published]

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: true

env:
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}

jobs:
  build:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./astro-blog
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: './astro-blog/package-lock.json'

      - name: Install dependencies
        run: |
          if [ -f package-lock.json ]; then
            npm ci
          else
            npm install
            echo "WARNING: No package-lock.json found. Run 'npm install' locally and commit the lockfile."
          fi

      - name: Build Astro site
        run: npm run build

      - name: Verify critical files
        run: |
          echo "=== Verifying critical SEO files ==="
          
          # Check sitemap.xml
          if [ ! -f "./dist/sitemap.xml" ]; then
            echo "ERROR: sitemap.xml not generated!"
            exit 1
          fi
          echo "✅ sitemap.xml exists ($(wc -c < ./dist/sitemap.xml) bytes)"
          
          # Check robots.txt
          if [ ! -f "./dist/robots.txt" ]; then
            echo "ERROR: robots.txt not generated!"
            exit 1
          fi
          echo "✅ robots.txt exists"
          
          # Verify no noindex in HTML
          if grep -rl "noindex" ./dist/ --include="*.html" 2>/dev/null | head -3; then
            echo "WARNING: Found noindex directives in HTML files!"
          else
            echo "✅ No noindex directives found"
          fi
          
          # Verify robots.txt doesn't block everything
          if grep -q "Disallow: /$" ./dist/robots.txt && ! grep -q "Allow:" ./dist/robots.txt; then
            echo "ERROR: robots.txt blocks all crawlers!"
            exit 1
          fi
          echo "✅ robots.txt allows crawling"
          
          # Verify CNAME exists
          if [ ! -f "./dist/CNAME" ]; then
            echo "ERROR: CNAME file missing!"
            exit 1
          fi
          echo "✅ CNAME exists: $(cat ./dist/CNAME)"
          
          # Show sitemap URL count
          URL_COUNT=$(grep -c "<loc>" ./dist/sitemap.xml || echo "0")
          echo "✅ Sitemap contains $URL_COUNT URLs"
          
          echo ""
          echo "=== robots.txt content ==="
          cat ./dist/robots.txt

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: './astro-blog/dist'

  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

  notify-indexnow:
    runs-on: ubuntu-latest
    needs: deploy
    steps:
      - name: Ping search engines
        run: |
          echo "Pinging search engines..."
          
          # Ping Google sitemap
          curl -s "https://www.google.com/ping?sitemap=https://blog.vistaceo.com/sitemap.xml" || true
          
          # Ping IndexNow (Bing/Yandex)
          curl -s "https://api.indexnow.org/indexnow?url=https://blog.vistaceo.com/sitemap.xml&key=8a7b6c5d4e3f2a1b0c9d8e7f6a5b4c3d" || true
          curl -s "https://www.bing.com/indexnow?url=https://blog.vistaceo.com/sitemap.xml&key=8a7b6c5d4e3f2a1b0c9d8e7f6a5b4c3d" || true
          
          echo "Done!"
